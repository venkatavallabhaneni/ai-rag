server:
  port: 8086

rag:
  chunkSize: 500
  overlap: 100   # 20% overlap

aigateway:
  baseurl: "http://localhost:4000"  # liteLLM AI Gateway
  embedding:
    model: "text-embedding-3-small"
  chat:
    model: "gpt-4o-mini"
  api-key: ${LITELLM_MASTER_KEY:local-test-key}  # matches lite-llm-config.yaml master_key

milvus:
  host: "localhost"
  port: 19530
  collection: "rag_chunks"
  vectorDim: 1536   # IMPORTANT: must match your embedding model dimension
  metricType: "COSINE"

litellm:
  baseUrl: "http://localhost:4000"
  chatPath: "/v1/chat/completions"
  model: "gpt-4o-mini"
  temperature: 0.2
  maxTokens: 400
  api-key: ${LITELLM_MASTER_KEY:local-test-key}
